---
title: "Playing with `tidymodels`"
output:
  html_document: 
    keep_md: yes
---


This is a brief notebook for exploration of the `tidymodels` package. It's meant to be a self motivated journey, filled with mistakes, questions, and generally a sandbox to experiment with the goal of understanding. Feel free to fork, branch, comment, etc, this GitHub file for your own usage.

## Libraries

First, let's load some libraries...

```{r libraryLoad}
# MASS has some conflicts - so load it first!
library( MASS )
# tidymodels is the point here
library( tidymodels )
# corrplot for correlation plots
library( corrplot )
```

## Boston House Price Dataset

For a first cut, I've started with the `Boston` data set from the `MASS` package. I've started here for several reasons:

1. It's a public data set, and the help file contains a data dictionary.  
2. It's relatively **clean** - all numeric variables (though some are really factors), no missing values, etc.
3. I've used it before, and will use it again in future presentations. Thus, having some ideas on how to utilise contemporary libraries would suit my ulterior motives.

```{r bostonHousePrice}
# Load the Boston house price data set from the MASS package
data( Boston, package='MASS' )
```

## Overview {.tabset .tabset-pills}

For the analysis, I'd like to complete the following:

1. Do a quick EDA on the variables (this is outside the `tidymodels` paradigm, but useful anyway!).
2. Select & scale variables for modelling
3. Split data into training/test for initial model construction, and again for $k$-fold cross validation.
4. Evaluate models based on training data.
    * Pick some metrics
    * Make some visualisations about those metrics
5. Evaluate models based on test data
    * Use metrics from above
    * Use visualisations from above
6. Given a value for $k$, perform cross validation with some metrics & plots.

### Quick EDA

```{r quickEDA}
# Use ggplot for density plots
plotData <- Boston %>%
  gather( key='Variable', value='Value' )

edaPlot <- ggplot( plotData ) +
  facet_wrap( ~Variable, scales='free' ) +
  geom_density( aes( x=Value, colour=Variable, fill=Variable ) ) +
  theme( legend.position='none' )
  
edaPlot
```


...and check out a `corrplot` of the variables:

```{r corrPlot}
corrplot( cor( Boston )
          , method = 'pie'
          , order='hclust'
          , addrect=3
          , addCoef.col = 'black'
          , number.cex=0.7 )
```

### Data Splitting

```{r dataSplit}
set.seed( 42 )
bostonSplit <- initial_split( Boston )
# Repeat EDA process
trainPlotData <- bostonSplit %>%
  training() %>%
  gather( key='Variable', value='Value' ) 

trainPlot <- ggplot( trainPlotData ) +
  facet_wrap( ~Variable, scales='free' ) +
  geom_density( aes( x=Value, colour=Variable, fill=Variable ) ) +
  theme( legend.position='none' )

trainPlot

corrplot( cor( training( bostonSplit ) )
          , method = 'pie'
          , order='hclust'
          , addrect=3
          , addCoef.col = 'black'
          , number.cex=0.7 )
```


### Variable Selection

I'll try a couple selections of variables - first by selecting a few from the `corrplot`, then using the automated `step_corr` option. 

This is where I'm starting the `tidymodels` experience - with the `recipe` terminology. I've included the formula notations here, as well as converting `chas` to a factor. I guess the same could be done to other variables (e.g. `rad`), but one thing at a time!

```{r varSelect}
myRecipe <- bostonSplit %>%
  training() %>%
  recipe( 'medv ~ chas + zn + rm + age' ) %>%
  # chas is adjecency to Charles river - set this to a factor
  step_num2factor( chas ) 

autoRecipe <- bostonSplit %>%
  training() %>%
  recipe( 'medv ~ .' ) %>%
  # chas is adjecency to Charles river - set this to a factor
  step_num2factor( chas ) %>%
  # Be sure not to remove things correlated to the outcome variable!
  step_corr( all_numeric(), -all_outcomes(), threshold=0.4 ) 

fullRecipe <- bostonSplit %>%
  training() %>%
  recipe( 'medv ~ .' ) %>%
  step_num2factor( chas )
```


Utilise the recipe to scale the training data.

```{r prepTrain}
myPrep <- prep( myRecipe )
autoPrep <- prep( autoRecipe )
```

Get the data back out for verification later.

```{r prepData}
myPrepData <- juice( myPrep )
autoPrepData <- juice( autoPrep )
```


### Training

Following preparation -- and with the "juiced" data for comparison -- let's build some models!

```{r modelTrain}
# Make a SVM object
svmObject <- svm_rbf( mode='regression', cost=10 ) %>%
  # Set the engine for clarity (Note there's only one as of this writing)
  set_engine( 'kernlab' )

# Also make a model for regularised regression
lmObject <- linear_reg(  ) %>%
  set_engine( 'lm' ) 

# Generic formula 
genFormula <- as.formula( 'medv ~ .' )
# Fit GLM and SVM to my variables and the auto correlated variables
mySVM <- svmObject %>%
  fit( genFormula, data=myPrepData  )

myLM <-  lmObject %>% 
  fit( genFormula, data=myPrepData )

autoSVM <- svmObject %>%
  fit( genFormula, data=autoPrepData )

autoGLM <- lmObject %>% 
  fit( genFormula, data=autoPrepData )

# Get a summary from the myGLM and autoSVM models
## ... I don't know how to do this. :(
```

### Testing

Following building models, let's evaluate some models!

```{r testModels}
# Do the transformations and data prep on the test data sets
myTestData <- myPrep %>%
  bake( testing( bostonSplit ) )


## I'm not sure why this doesn't work...
# autoTestData <- autoPrep %>%
#   testing( bostonSplit ) %>%
#   bake()

# But this one does
autoTestData <- autoPrep %>%
  bake( testing( bostonSplit ) )

# Generate predictions
myResultsData <-myTestData %>%
  bind_cols( mySVM %>%
  predict( myTestData ) %>%
  rename( svmPred=.pred ) 
  ) %>%
  bind_cols( myLM %>%
               predict( myTestData ) %>%
               rename( lmPred=.pred ) 
             )
## Could do this with auto, but won't (for now)
```

```{r metricSummary}
myMetrics <- bind_rows( metrics( myResultsData, medv, lmPred ) %>% mutate( model='LM' ) 
                        , metrics( myResultsData, medv, svmPred ) %>% mutate( model='SVM' ) ) %>%
  # THere's multiple "select" functions running around - scope it
  dplyr::select( -.estimator )

metricPlot <- ggplot( myMetrics ) +
  geom_bar( aes( x=.metric, y=.estimate, fill=model, colour=model ), stat='identity', position='dodge' ) +
  scale_colour_brewer( type='qual' )

metricPlot
```

### Cross-validation

**Not keeping the md file seems to crash this for me. If it crashes on knitting, try keeping the markdown file.**

I found an article on Medium that has a good intro to CV with `tidymodels`. I've leveraged that in the below, only splitting the full data set rather than the training set.

```{r crossValidation, cache=FALSE, results='asis'}
crossValData <- vfold_cv( Boston, v=10 ) %>%
  # Rename the values to include the fact that I'll be using them for cross validation
  rename( cvSplit=splits, cvID=id )

# Build a list of test cases
fullCVData <- bind_rows(
  crossValData %>% mutate( cvRecipe=list( autoRecipe ), cvModel=list( svmObject ), modelName='autoSVM' )
  , crossValData %>% mutate( cvRecipe=list( autoRecipe ), cvModel=list( lmObject ), modelName='autoLM' )
  , crossValData %>% mutate( cvRecipe=list( myRecipe ), cvModel=list( lmObject ), modelName='myLM' )
  , crossValData %>% mutate( cvRecipe = list( myRecipe ), cvModel=list( svmObject ), modelName='mySVM' )
)

# Build a function to evaluate the parts - note the names in the function match the names in the list!
cvFun <- function( cvSplit, cvID, cvRecipe, cvModel, modelName='Default' ){
  # Note that this takes a single recipe (at a time)
  prepRecipe <- cvSplit %>% training() %>% prep( x=cvRecipe, training=. )
  trainData <- cvSplit %>% training() %>% bake( prepRecipe, new_data=. )
  testData <- cvSplit %>% testing() %>% bake( prepRecipe, new_data=. )
  
  modelObject <- cvModel %>% fit( formula( prepRecipe ), data=trainData )
  
  testResults <- modelObject %>% predict( new_data=testData ) %>% bind_cols( testData  )
  
  testMetrics <- testResults %>%
    metrics( medv, .pred ) %>%
    dplyr::select( -.estimator ) %>%
    mutate( ID=cvID, Name=modelName )
  
  return( testMetrics )
}

# Get the results through mapping
fullCVResults <- pmap_dfr( fullCVData, cvFun ) %>%
  # Get rid of pesky dots
  rename_all( list( ~sub( pattern='.', replacement='', fixed=TRUE, x=. ) ) )

cvPlot <- ggplot( fullCVResults ) +
  geom_boxplot( aes( x=metric, y=estimate, colour=Name ), position='dodge' )

cvPlot
```